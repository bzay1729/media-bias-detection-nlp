# -*- coding: utf-8 -*-
"""bias_classification_notebook_by_hirenBijay.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1daAnDGvmOjuuPgkB3yxEWpKCHwsAfGVU

# Political Bias Classification with BERT

This notebook trains a XLM-RoBERTa model to classify political bias in news articles.

## Experimental Setup
- **Training Data**: AllSides balanced news headlines dataset (~21K samples)
  - Shorter texts (headlines + snippets)
  - Labels: left, center, right
- **Test Data**: Gold OpenAI annotations (300 samples)
  - Longer texts (multiple snippets)
  - Labels: Left, Center, Right
  
This setup tests if a model trained on shorter texts can generalize to longer, more complex articles.

## 1. Import Required Libraries
"""

import pandas as pd
import numpy as np
import torch
from datasets import Dataset
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from transformers import TrainingArguments, Trainer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, f1_score, confusion_matrix, classification_report
import warnings
warnings.filterwarnings('ignore')

print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")

"""## 2. Load and Explore Training Dataset (AllSides)"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd

# Correct file path inside Google Drive
file_path = "/content/drive/MyDrive/NLP_Project_Dataset/allsides_balanced_news_headlines-texts.csv"

# Load AllSides training data
train_df = pd.read_csv(file_path)

print(f"Training dataset shape: {train_df.shape}")
print(f"\nColumns: {train_df.columns.tolist()}")
print(f"\nFirst 3 rows:")
print(train_df.head(3))
print(f"\nBias rating distribution:")
print(train_df['bias_rating'].value_counts())
print(f"\nMissing values:")
print(train_df.isnull().sum())

# Check text length
train_df['text_length'] = train_df['text'].fillna('').str.len()
print(f"\nText length statistics (AllSides):")
print(train_df['text_length'].describe())

"""Once the file is uploaded, you can re-run the cell where the error occurred to proceed.

## 3. Load and Explore Test Dataset (Gold OpenAI)
"""

# Correct file path inside Google Drive
test_file_path = "/content/drive/MyDrive/NLP_Project_Dataset/gold_openai_annotations_all_batches_1_30.csv"

# Load Gold OpenAI test data
test_df = pd.read_csv(test_file_path)

print(f"Test dataset shape: {test_df.shape}")
print(f"\nColumns: {test_df.columns.tolist()}")
print(f"\nFirst 3 rows:")
print(test_df[['docid', 'title', 'ManualBiasLabel']].head(3))
print(f"\nManualBiasLabel distribution:")
print(test_df['ManualBiasLabel'].value_counts())
print(f"\nMissing values:")
print(test_df.isnull().sum())

# Combine snippets into text
test_df['text'] = (
    test_df['snippet1'].fillna('') + ' ' +
    test_df['snippet2'].fillna('') + ' ' +
    test_df['snippet3'].fillna('')
).str.strip()

test_df['text_length'] = test_df['text'].str.len()
print(f"\nText length statistics (Gold OpenAI):")
print(test_df['text_length'].describe())

"""## 4. Preprocess Training Data (AllSides)

Standardize labels to match: `Left`, `Center`, `Right`
"""

# Standardize labels: left -> Left, center -> Center, right -> Right
train_df['ManualBiasLabel'] = train_df['bias_rating'].str.capitalize()

# Remove rows with missing text
train_df = train_df[train_df['text'].notna()].copy()
train_df = train_df[train_df['text'].str.len() > 10].copy()  # Remove very short texts

print(f"Training data after cleaning: {train_df.shape}")
print(f"Label distribution:")
print(train_df['ManualBiasLabel'].value_counts())

# Split into train and validation
train_data, val_data = train_test_split(
    train_df[['text', 'ManualBiasLabel']],
    test_size=0.2,
    stratify=train_df['ManualBiasLabel'],
    random_state=42
)

print(f"\nTrain split: {train_data.shape}")
print(f"Validation split: {val_data.shape}")

"""## 5. Preprocess Test Data (Gold OpenAI)

Ensure the test data has the same label format and remove any missing values.
"""

# Test data already has 'text' column created and ManualBiasLabel
test_data = test_df[['text', 'ManualBiasLabel']].copy()

# Remove rows with missing text
test_data = test_data[test_data['text'].notna()].copy()
test_data = test_data[test_data['text'].str.len() > 10].copy()

print(f"Test data after cleaning: {test_data.shape}")
print(f"Label distribution:")
print(test_data['ManualBiasLabel'].value_counts())

"""## 6. Encode Labels

Use `LabelEncoder` to convert text labels to integers consistently across train, val, and test sets.
"""

# Fit label encoder on training data
label_encoder = LabelEncoder()
train_data['labels'] = label_encoder.fit_transform(train_data['ManualBiasLabel'])
val_data['labels'] = label_encoder.transform(val_data['ManualBiasLabel'])
test_data['labels'] = label_encoder.transform(test_data['ManualBiasLabel'])

print("Label mapping:", dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_))))
print(f"\nNumber of classes: {len(label_encoder.classes_)}")

# Convert to HuggingFace Datasets
train_dataset = Dataset.from_pandas(train_data[['text', 'labels']].reset_index(drop=True))
val_dataset = Dataset.from_pandas(val_data[['text', 'labels']].reset_index(drop=True))
test_dataset = Dataset.from_pandas(test_data[['text', 'labels']].reset_index(drop=True))

print(f"\nDataset sizes:")
print(f"  Train: {len(train_dataset)}")
print(f"  Val: {len(val_dataset)}")
print(f"  Test: {len(test_dataset)}")

"""## 7. Load Tokenizer and Model

Using XLM-RoBERTa base for multilingual and robust text classification.
"""

model_name = "FacebookAI/xlm-roberta-base"

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)

# Load model for sequence classification
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=len(label_encoder.classes_)
)

print(f"Model loaded: {model_name}")
print(f"Number of parameters: {sum(p.numel() for p in model.parameters()):,}")

"""## 8. Tokenize Datasets

Tokenize all text data with padding and truncation to max_length of 256 tokens.
"""

def tokenize(batch):
    return tokenizer(
        batch["text"],
        padding="max_length",
        truncation=True,
        max_length=512
    )

# Tokenize all datasets
train_dataset = train_dataset.map(tokenize, batched=True)
val_dataset = val_dataset.map(tokenize, batched=True)
test_dataset = test_dataset.map(tokenize, batched=True)

# Set format for PyTorch
train_dataset.set_format("torch")
val_dataset.set_format("torch")
test_dataset.set_format("torch")

print("Tokenization complete!")
print(f"Train dataset columns: {train_dataset.column_names}")

"""## 9. Define Evaluation Metrics

Compute accuracy and macro-averaged F1 score for model evaluation.
"""

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = logits.argmax(axis=-1)
    return {
        "accuracy": accuracy_score(labels, preds),
        "f1_macro": f1_score(labels, preds, average="macro")
    }

"""## 10. Configure Training Arguments

Set up hyperparameters for training with the Hugging Face Trainer.
"""

training_args = TrainingArguments(
    output_dir="./allsides_trained_model",
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_strategy="steps",
    logging_steps=100,
    learning_rate=3e-5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=10,
    weight_decay=0.01,
    load_best_model_at_end=True,
    metric_for_best_model="f1_macro",
    report_to="none",
    save_total_limit=2,
    fp16=True,
    optim="adamw_torch",
    lr_scheduler_type="linear",
    gradient_accumulation_steps=2

)

print("Training configuration:")
print(f"  Batch size: {training_args.per_device_train_batch_size}")
print(f"  Learning rate: {training_args.learning_rate}")
print(f"  Epochs: {training_args.num_train_epochs}")
print(f"  Output directory: {training_args.output_dir}")

"""# For Baseline"""

# 11. Baseline Evaluation (Pre-Fine-Tuning)
# Evaluate the XLM-RoBERTa model before any fine-tuning

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score
import numpy as np

print("\n==== BASELINE EVALUATION (unfine-tuned model) ====")

# Use a separate Trainer instance for baseline evaluation
baseline_trainer = Trainer(
    model=model,                 # this is the pre-trained, NOT yet fine-tuned model
    args=training_args,
    eval_dataset=val_dataset,    # we’ll first evaluate on the validation set
    processing_class=tokenizer,
    compute_metrics=compute_metrics
)

# 1) Validation metrics (before training)
baseline_val_results = baseline_trainer.evaluate()
print("\n[BASELINE] Validation metrics:")
for key, value in baseline_val_results.items():
    print(f"  {key}: {value:.4f}")

# 2) Test metrics + detailed report (before training)
baseline_test_predictions = baseline_trainer.predict(test_dataset)
baseline_test_logits = baseline_test_predictions.predictions
baseline_test_labels = baseline_test_predictions.label_ids
baseline_test_preds = np.argmax(baseline_test_logits, axis=1)

baseline_test_acc = accuracy_score(baseline_test_labels, baseline_test_preds)
baseline_test_f1 = f1_score(baseline_test_labels, baseline_test_preds, average="macro")

print("\n[BASELINE] Test metrics (Gold OpenAI):")
print(f"  accuracy:  {baseline_test_acc:.4f}")
print(f"  f1_macro:  {baseline_test_f1:.4f}")

print("\n[BASELINE] Test classification report:")
print(classification_report(
    baseline_test_labels,
    baseline_test_preds,
    target_names=label_encoder.classes_
))

print("[BASELINE] Test confusion matrix:")
print(confusion_matrix(baseline_test_labels, baseline_test_preds))

print("\n==== End of BASELINE evaluation ====\n")

"""## CM For baseline"""

import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix

# Compute confusion matrix
cm_baseline = confusion_matrix(baseline_test_labels, baseline_test_preds)
class_names = label_encoder.classes_  # ['Center', 'Left', 'Right']

# Plot heatmap
plt.figure(figsize=(7, 6))
sns.heatmap(
    cm_baseline,
    annot=True,
    fmt="d",
    cmap="Reds",
    xticklabels=class_names,
    yticklabels=class_names
)

plt.title("Confusion Matrix — Baseline XLM-RoBERTa (Unfine-Tuned)", fontsize=14)
plt.xlabel("Predicted Label", fontsize=12)
plt.ylabel("True Label", fontsize=12)
plt.show()

"""## 11. Initialize Trainer and Start Training

Create the Trainer and begin fine-tuning on the AllSides dataset.
"""

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    processing_class=tokenizer,
    compute_metrics=compute_metrics
)

print("Starting training...")
print(f"Training on {len(train_dataset)} samples")
print(f"Validating on {len(val_dataset)} samples")
print("-" * 50)

# Train the model
trainer.train()

print("\nTraining complete!")

"""## 12. Evaluate on Test Set (Gold OpenAI)

Now evaluate the model trained on AllSides data against the Gold OpenAI test set.
"""

print("Evaluating on Gold OpenAI test set...")
test_results = trainer.evaluate(test_dataset)

print("\n" + "="*50)
print("TEST SET RESULTS (Gold OpenAI)")
print("="*50)
for key, value in test_results.items():
    print(f"{key}: {value:.4f}")

"""## 13. Generate Predictions and Confusion Matrix

Get detailed per-class performance metrics.
"""

# Get predictions
preds_output = trainer.predict(test_dataset)
preds = np.argmax(preds_output.predictions, axis=1)
labels = preds_output.label_ids

# Confusion matrix
print("\n" + "="*50)
print("CONFUSION MATRIX")
print("="*50)
cm = confusion_matrix(labels, preds)
print(cm)

# Classification report
print("\n" + "="*50)
print("CLASSIFICATION REPORT")
print("="*50)
print(classification_report(labels, preds, target_names=label_encoder.classes_))

import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np

# Your confusion matrix (already computed)
cm = confusion_matrix(labels, preds)
class_names = label_encoder.classes_   # ['Center', 'Left', 'Right']

# --- PLOT CONFUSION MATRIX ---
plt.figure(figsize=(7, 6))
sns.heatmap(
    cm,
    annot=True,
    fmt="d",
    cmap="Blues",
    xticklabels=class_names,
    yticklabels=class_names
)

plt.title("Confusion Matrix — Fine-Tuned RoBERTa", fontsize=14)
plt.xlabel("Predicted Label", fontsize=12)
plt.ylabel("True Label", fontsize=12)
plt.show()

"""## 14. Analysis and Observations

### Key Findings:
- **Training Set (AllSides)**: ~21K samples with shorter texts
- **Test Set (Gold OpenAI)**: 300 samples with longer, more complex texts

### Expected Challenges:
1. **Domain Shift**: Training on shorter headline-style texts vs testing on longer article snippets
2. **Text Length Mismatch**: AllSides texts are typically shorter than Gold OpenAI texts
3. **Class Imbalance**: Both datasets have imbalanced classes, but distributions differ

### Improvements to Try:
- Add more training data or use data augmentation
- Experiment with different max_length settings (longer context)
- Try class weighting to handle imbalance
- Fine-tune on a mix of both short and long texts
- Use domain adaptation techniques

## 15. Save the Model (Optional)

Uncomment to save the trained model for later use.
"""

# Uncomment to save the model and tokenizer
# model.save_pretrained("./final_bias_classifier")
# tokenizer.save_pretrained("./final_bias_classifier")
# print("Model saved to ./final_bias_classifier")

print("Notebook complete! Review the results above.")